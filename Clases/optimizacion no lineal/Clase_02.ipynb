{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "4169e6f9-031a-415e-a468-fe516a4931e9",
                "collapsed": true,
                "id": "ehCT2lXP3Sty",
                "pycharm": {
                    "is_executing": false
                }
            },
            "source": [
                "### Introducción a la Investigación Operativa y la Optimización\n",
                "\n",
                "### • Algoritmos de descenso\n",
                "\n",
                "**Nazareno Faillace Mullen - Departamento de Matemática, FCEN, UBA**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "azdata_cell_guid": "84e13600-5b2d-4965-9767-7e6d3c5d8ca0",
                "id": "JQs7GAsMKjMe",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "import numpy as np"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "azdata_cell_guid": "ad6c0284-32d6-4bb0-be87-13ec487f6462",
                "id": "lHNwpyO3LmiY",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "from numpy.linalg import norm\n",
                "\n",
                "def derivada_parcial(func,x,i):\n",
                "  \"\"\"\n",
                "  Aproxima la i-esima derivada parcial de la función en x, utilizando diferencias centradas.\n",
                "  func: función a la que se le desea calcular la i-esima derivada parcial (function)\n",
                "  x: punto en el cual se desea calcular la i-esima derivada parcial (array de numpy)\n",
                "  i: indice de la coordenada parcial (int)\n",
                "  \"\"\"\n",
                "  h = 0.1\n",
                "  e_i = np.zeros(x.shape[0])  # np.zeros(len(x))\n",
                "  e_i[i] = 1\n",
                "  z = (func(x + h*e_i) - func(x - h*e_i))/(2*h)\n",
                "  h = h/2\n",
                "  y = (func(x + h*e_i) - func(x - h*e_i))/(2*h)\n",
                "  error = norm(y-z)\n",
                "  eps = 1e-8\n",
                "  while error>eps and (y != np.nan) and (y != np.inf) and y!= 0:\n",
                "      error = norm(y-z)\n",
                "      z = y\n",
                "      h = h/2\n",
                "      y = (func(x + h*e_i) - func(x - h*e_i))/(2*h)\n",
                "  return z\n",
                "\n",
                "def gradiente(func,x):\n",
                "  \"\"\"\n",
                "  Aproxima el gradiente de la función en x.\n",
                "  func: función a la que se le desea calcular el gradiente (function)\n",
                "  x: punto en el cual se desea calcular el gradiente (array de numpy)\n",
                "  \"\"\"\n",
                "  grad = np.empty(x.shape[0]) # np.zeros(x.shape[0])\n",
                "  for i in range(x.shape[0]):\n",
                "    grad[i] = derivada_parcial(func, x, i)\n",
                "  return grad"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "7d5ef8e5-c3b0-43fa-a024-f2c399d305ce",
                "id": "wW58Fq7H3St2",
                "pycharm": {}
            },
            "source": [
                "# Algoritmos de descenso"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "c931a7f8-201f-43ca-91fe-d4ead470fa56",
                "id": "2dSE3Bo5BS3o"
            },
            "source": [
                "![](https://drive.google.com/uc?export=view&id=1pvqcG3ePvBJvfAn5cE14Fx7SNS2LeL-F)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "a73ed029-5d7c-4dfb-af8f-d94accf5df20",
                "id": "7xwQCOaJ3St3",
                "pycharm": {}
            },
            "source": [
                "_Idea_: a partir de un punto obtenido, escoger una dirección para dar el próximo paso\n",
                "\n",
                "__Definición (dirección de descenso)__: sean $f:\\mathbb{R}^n \\rightarrow \\mathbb{R}$, $\\bar{x}\\in\\mathbb{R}^n$ y $d\\in\\mathbb{R}^n-\\{0\\}$, diremos que $d$ es una dirección de descenso para $f$ a partir de $\\bar{x}$ si existe $\\delta>0$ tal que $f(\\bar{x}+td)<f(\\bar{x}) \\quad \\forall t\\in(0,\\delta)$\n",
                "\n",
                "__Teorema__: Si $\\nabla f(\\bar{x})d < 0$, entonces $d$ es dirección de descenso para $f$ a partir de $\\bar{x}$"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "6eb1b087-acc7-410b-b9bf-706374237cf5",
                "id": "e48qaab23St5",
                "pycharm": {}
            },
            "source": [
                "## __Algoritmo de descenso básico__\n",
                "\n",
                "Dados: $f,\\; x_0 \\in \\mathbb{R}^n,\\; \\varepsilon>0,\\; k_{MAX}>0$ <br>\n",
                "k = 0 <br>\n",
                "REPETIR mientras $\\nabla f(x_k) > \\varepsilon$ y $k<k_{MAX}$ : <br>\n",
                "&nbsp;&nbsp;&nbsp;&nbsp; Calcular $d_k$ tal que $\\nabla f(x_k)^Td_k < 0$ <br>\n",
                "&nbsp;&nbsp;&nbsp;&nbsp; Escoger $t_k>0$ tal que $f(x_k+t_kd_k)<f(x_k)$ <br>\n",
                "&nbsp;&nbsp;&nbsp;&nbsp; Hacer $x_{k+1}=x_k + t_kd_k$ <br>\n",
                "&nbsp;&nbsp;&nbsp;&nbsp; $k = k+1$\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "b38bc21f-df8e-41c9-85e4-a3f12b837fa4",
                "id": "ErRH2W6fH7u7"
            },
            "source": [
                "## Método del gradiente - Funciones cuadráticas"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "bdd65fd1-3365-4dd1-a299-add37fb2c5fe",
                "id": "6eDCqb3TP_1b",
                "pycharm": {}
            },
            "source": [
                "Este método toma como dirección de descenso:\n",
                "$$d = -\\nabla f(x)$$\n",
                "Observar que, si $\\nabla f(x) \\neq 0$, efectivamente $d$ es dirección de descenso:\n",
                "$$\\nabla f(x)^T d = -\\left\\Vert \\nabla f(x) \\right\\Vert ^2 < 0$$"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "1d88d48b-67d3-4e0c-a937-4627846271cf",
                "id": "lf6eZFFtH7u9"
            },
            "source": [
                "Las funciones cuadráticas pueden escribirse en la forma:\n",
                "\n",
                "$$f(x) = \\frac{1}{2}x^T A x + bx + c$$\n",
                "\n",
                "En este caso, el gradiente se calcula fácilmente: <br>\n",
                "$$\\nabla f(x) = Ax + b$$"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "c7561809-aced-4631-ac3e-8ba73170467f",
                "id": "oZB10aH7H7u9"
            },
            "source": [
                "En el método del gradiente, $d_k = -\\nabla f(x_k)$\n",
                "\n",
                "Si $A$ es definida positiva, se puede demostrar que $\\varphi(t) = f(x_k + td_k)$ alcanza mínimo en:\n",
                "$$ t^\\ast = \\dfrac{d_k^T d_k}{d_k^T A d_k}$$\n",
                "\n",
                "Entonces, en cada iteración se puede calcular la longitud del paso óptimo."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "874e3cdd-8970-4c75-8426-cdd8b1028230",
                "id": "LFESYLqRH7u-"
            },
            "source": [
                "__Pseudocódigo de Método de gradiente para funciones cuadráticas__\n",
                "\n",
                "metodo_gradiente_cuad($A$,$b$,$x$, $max\\_iter$):<br>\n",
                "&nbsp; &nbsp; $k$ $\\leftarrow$ $0$ <br>\n",
                "&nbsp; &nbsp; $d$ $\\leftarrow$ $-Ax-b$ &nbsp; &nbsp; `# Dirección del primer paso` <br>\n",
                "&nbsp; &nbsp; while $k\\leq max\\_iter$ and $\\lVert d\\rVert$ $> 10^{-8}$: <br>\n",
                "&nbsp; &nbsp; &nbsp; &nbsp; $t \\leftarrow \\dfrac{d^T d}{d^T A d}$ &nbsp; &nbsp; `# Determino la longitud del paso` <br>\n",
                "&nbsp; &nbsp; &nbsp; &nbsp; $x$ $\\leftarrow$ $x + td$ &nbsp; &nbsp; `# Calculo el siguiente punto de la iteración (\"doy el paso\")`<br>\n",
                "&nbsp; &nbsp; &nbsp; &nbsp; $d$ $\\leftarrow$ $- Ax-b$  &nbsp; &nbsp; `# Dirección del próximo paso` <br>\n",
                "&nbsp; &nbsp; &nbsp; &nbsp; $k$ $\\leftarrow$ $k+1$ <br>\n",
                "&nbsp; &nbsp; DEVOLVER $x$\n",
                "\n",
                "**Obs:** utilizar `np.linalg.norm(v)` para calcular la norma del vector `v`"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "18b75808-618b-4bbf-ad17-ba85956b6f4c",
                "id": "L2ahQDByH7u_"
            },
            "source": [
                "### Ejercicios\n",
                "\n",
                "1. Implementar el Método del gradiente para funciones cuadráticas en base al pseudocódigo anterior. Además de devolver la aproximación al mínimo, que imprima la cantidad de iteraciones.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 60,
            "metadata": {
                "azdata_cell_guid": "4b208919-07fc-4db3-973b-c19871170bf8",
                "id": "S9BEzp_6WiwF",
                "language": "python",
                "tags": []
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "defined\n"
                    ]
                }
            ],
            "source": [
                "def metodo_gradiente_cuad(A, b, x, eps=1e-5, k_max=10000):\n",
                "  \"\"\"\n",
                "  Aplica el método del gradiente.\n",
                "  func: funcion a optimizar (function)\n",
                "  x: punto inicial (numpy.array)\n",
                "  eps: valor de tolerancia para la norma del gradiente (float)\n",
                "  k_max: limite de iteraciones (int)\n",
                "  \"\"\"\n",
                "  k = 0\n",
                "  d = -A@x - b\n",
                "  while k <= k_max and np.linalg.norm(d) > eps:\n",
                "    t = (d@d)/(d@A@d)\n",
                "    x = x + t*d\n",
                "    d = (-A)@x - b\n",
                "    k += 1\n",
                "  return x, k\n",
                "\n",
                "print(\"defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "39536d3d-d0d5-498b-b841-ee82dcd966e0",
                "id": "KeGGMeNgWjHd"
            },
            "source": [
                "2. Para la función cuadrática $f(x)=\\frac{1}{2}x^TAx$ con $A$ dada más abajo, correr el Método del Gradiente: <br>\n",
                "a) como fue implementado en el punto 1 <br>\n",
                "b) con $\\frac{1}{2}$ de longitud del paso óptimo <br>\n",
                "c) en cada iteración multiplicar $t$  longitud de paso por un número aleatorio en (0,1] (`np.random.rand()` devuelve un float aleatorio) <br>\n",
                "Probar con $x_0$ el vector de 1's (`np.ones(10)`) y un máximo de 10000 iteraciones. ¿Cuál se desempeña mejor?\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 56,
            "metadata": {
                "azdata_cell_guid": "253cf61f-62f1-4780-b686-56f08d45674c",
                "id": "BOjCV_FAW5Dn",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "A = np.array([[8, 3, 3, 6, 5, 4, 4, 3, 6, 3],\n",
                "             [3, 4, 2, 2, 2, 1, 3, 3, 3, 2],\n",
                "             [3, 2, 5, 2, 1, 2, 4, 2, 4, 1],\n",
                "             [6, 2, 2, 6, 3, 2, 4, 2, 4, 2],\n",
                "             [5, 2, 1, 3, 5, 4, 1, 2, 4, 3],\n",
                "             [4, 1, 2, 2, 4, 5, 1, 2, 5, 2],\n",
                "             [4, 3, 4, 4, 1, 1, 6, 2, 4, 2],\n",
                "             [3, 3, 2, 2, 2, 2, 2, 4, 4, 2],\n",
                "             [6, 3, 4, 4, 4, 5, 4, 4, 8, 3],\n",
                "             [3, 2, 1, 2, 3, 2, 2, 2, 3, 4]])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 61,
            "metadata": {},
            "outputs": [],
            "source": [
                "xx=np.ones(10)\n",
                "minima, iteraciones = metodo_gradiente_cuad(A, b=np.zeros(10), x=xx)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 62,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Minimo: [ 0.00055401  0.00357508  0.00086696  0.00227464 -0.00513704  0.00294843\n",
                        " -0.00349607 -0.00247662 -0.00041048  0.00211462]\n",
                        "valor de funcion: 156.5\n",
                        "Iteraciones: 10001\n"
                    ]
                }
            ],
            "source": [
                "print(f\"Minimo: {minima}\\nvalor de funcion: {(0.5*xx@A@xx)}\\nIteraciones: {iteraciones}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "5c29d26a-c661-4712-8426-371421f255c5",
                "id": "5jpAhfOr3St6",
                "pycharm": {}
            },
            "source": [
                "# Si $f$ no es cuadrática, ¿cuánto avanzo? - Métodos de búsqueda unidireccional"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "4a4cafc1-338f-432f-ad9e-9eb58ec1fe40",
                "id": "VtgPDy153St6",
                "pycharm": {}
            },
            "source": [
                "Dados $x,d\\in\\mathbb{R}^n$, lo que nosotros querríamos hacer es resolver el siguiente problema:\n",
                "\n",
                "minimizar $f(x+td)$\n",
                "\n",
                "\n",
                "sujeto a: $t\\geq0$\n",
                "\n",
                "Naturalmente, es un objetivo ambicioso pues no resulta una tarea fácil salvo que $f$ cumpla con características muy específicas. Veremos algoritmos que permiten aproximar a la solución de ese problema."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "3a1515aa-fddd-4646-b7a9-8b2c5b05d542",
                "id": "3Q3Yg2Tc3St_",
                "pycharm": {}
            },
            "source": [
                "## Búsqueda inexacta - Condición de Armijo"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "58ed39c2-c98a-42a2-9bec-48c754b9a30d",
                "id": "CFdUnIm63St_",
                "pycharm": {}
            },
            "source": [
                "A diferencia de la sección áurea, este algoritmo no busca minimizar $\\varphi(t)$ sino encontrar $t$ tal que haya una buena reducción en la función objetivo. Más específicamente, dados $\\bar{x}\\in\\mathbb{R}^n$, $d$ dirección de descenso y $\\eta\\in(0,1)$ busca $\\bar{t}$ tal que:\n",
                "$$f(\\bar{x}+\\bar{t}d) \\leq f(\\bar{x})+\\eta\\bar{t}\\nabla f(\\bar{x})^Td$$\n",
                "es decir, la reducción debe ser proporcional al tamaño del paso."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "9ab537b6-cf87-4c81-a296-a32fcbbc46fe",
                "id": "v-liVb3K3SuA",
                "pycharm": {}
            },
            "source": [
                "### Interpretación gráfica (Armijo)\n",
                "\n",
                "![](https://drive.google.com/uc?export=view&id=1uIJWhVdc19sqet0zVkCrX_v_jU5cpVAw)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "9c0de9bb-a8bd-4b0c-a5b0-6e7ccba8198f",
                "id": "LbbvVtKg3SuA",
                "pycharm": {}
            },
            "source": [
                "__Algoritmo de Búsqueda de Armijo__\n",
                "\n",
                "Dados: $f\\colon\\mathbb{R}^n\\rightarrow\\mathbb{R},\\;\\bar{x}\\in\\mathbb{R}^n$, $d\\in \\mathbb{R^n}$ dirección de descenso, $\\gamma,\\;\\eta\\in(0,1)$<br>\n",
                "\n",
                "$t\\leftarrow 1$<br>\n",
                "REPETIR mientras $f(\\bar{x}+td) > f(\\bar{x})+\\eta t\\nabla f(\\bar{x})^Td$ <br>\n",
                "&nbsp;&nbsp;&nbsp;&nbsp; $t\\leftarrow \\gamma t$<br>\n",
                "DEVOLVER $t$\n",
                "\n",
                "Valores de referencia para los parámetros: $\\gamma = 0.7,\\; \\eta=0.45$"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "520ccf95-21ba-45ab-9b54-5d9ac8694533",
                "id": "tEpYG5KEAIxE"
            },
            "source": [
                "### Ejercicio\n",
                "\n",
                "1. Implementar la funcion `armijo` que encuentre un punto en la dirección $d$ que cumpla con la condición de Armijo"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "metadata": {
                "azdata_cell_guid": "5a976061-a54b-470e-aba6-2f91cddc399b",
                "id": "enkBEWMYATi_",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "def armijo(func, x, d, gamma=0.7, eta=0.45):\n",
                "  \"\"\"\n",
                "  Aplica la búsqueda de Armijo para hallar un valor de t que cumpla la condición de Armijo para f(x+td)\n",
                "  func: funcion a optimizar (function)\n",
                "  x: punto a partir del cual buscar un minimizador (numpy.array)\n",
                "  d: direccion en la que se busca un minimizador (numpy.array)\n",
                "  gamma: parámetro que regula cuánto se achica t (float)\n",
                "  eta: parámetro que regula la pendiente de la condición de Armijo (int o float)\n",
                "  \"\"\"\n",
                "  t = 1\n",
                "  ls = func(x + t*d)\n",
                "  rs = func(x) + (eta*t)*(gradiente(func, x)@d)\n",
                "  while ls > rs:\n",
                "    t = gamma*t\n",
                "  return t"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "74cd5710-6106-4611-a160-476504517b87",
                "id": "DeOiCj5IA_sP"
            },
            "source": [
                "2. Poner a prueba la función implementada en el ítem anterior para hallar un valor de $t$ que cumpla la condición de Armijo, para $f(x)=x_1^2+x_2$ desde el punto $(1, 0)$ en la dirección $d=(-2,-1)$."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 49,
            "metadata": {
                "azdata_cell_guid": "0e5b536d-0656-40f3-abb7-927f0d1660c4",
                "id": "y5XqM5xoBHkL",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "def f(x1):\n",
                "  return x1[0]**2 + x1[1]\n",
                "\n",
                "x = np.array([1,0])\n",
                "dire = np.array([-2,-1])\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "a8b0397e-f26d-413b-bccd-ae3a708d32c5",
                "id": "-8R0c3vV3SuB",
                "pycharm": {}
            },
            "source": [
                "## Búsqueda inexacta - Condiciones de Wolfe"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "e3b4c853-3b67-4be4-bf72-289b8887fa9c",
                "id": "SsL4Prvy3SuB",
                "pycharm": {}
            },
            "source": [
                "La condicion de Armijo impone cotas a cuan grande puede ser el paso en la direccion $d$. Sin embargo, puede ocurrir que el paso sea tan pequeño que $x^k$ no converja a un minimizador local.\n",
                "\n",
                "Por ejemplo, si $f(x)=x^2$, si comenzamos en $x_0=2$, $d=-1$ es dirección de descenso en $x_k = 1+2^{-k}$ y $t_k = 2^{-k-1}$ cumple con la condición y efectivamente se logra un descenso en $f$. Sin embargo, $x_k\\rightarrow 1$ que no es el minimizador de $f$. No se converge al mínimo pues los pasos son muy pequeños.\n",
                "\n",
                "Entonces, necesitamos otra condición que acote inferiormente a $t$.\n",
                "\n",
                "Wolfe agrega otra condición sobre $t$, la condición de curvatura:\n",
                "$$\\nabla f (\\bar{x} + \\bar{t}d)^Td \\geq \\zeta \\nabla f(\\bar{x})^T d$$\n",
                "donde $\\zeta\\in(\\eta,1)$ con $\\eta$ siendo la constante de la condición de Armijo. El lado izquierdo es $\\varphi'(\\bar{t})$, por lo que la condición impone que la pendiente de $\\varphi$ en $\\bar{t}$ sea mayor que $\\zeta$ veces la pendiente inicial.\n",
                "\n",
                "Si $\\varphi'$ es muy negativa $\\Rightarrow$ se puede decrecer mucho en esta dirección <br>\n",
                "Si $\\varphi'$ no es muy negativa o es positiva $\\Rightarrow$ terminar la búsqueda lineal, no se pueden lograr (muchas) mejoras"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "6fb10d2f-8824-4d20-b229-129d04dcaf02",
                "id": "VKIX-H1x3SuB",
                "pycharm": {}
            },
            "source": [
                "__Condiciones de Wolfe:__\n",
                "\n",
                "$$\\begin{array}{rcl} f(\\bar{x}+\\bar{t}d) &\\leq& f(\\bar{x})+c_1\\bar{t}\\nabla f(\\bar{x})^Td \\\\\n",
                "\\nabla f (\\bar{x} + \\bar{t}d)^Td &\\geq& c_2 \\nabla f(\\bar{x})^T d \\end{array}$$\n",
                "Con $0<c_1<c_2<1$."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "ea76532f-624e-427c-9cba-506028c8e6c5",
                "id": "9y6jilnB3SuB",
                "pycharm": {}
            },
            "source": [
                "__Algoritmo de Búsqueda de Wolfe__\n",
                "\n",
                "Dados: $f,\\; \\bar{x}\\in\\mathbb{R}^n,\\; d$ dirección de descenso,$\\; 0<c_1<c_2<1$ <br>\n",
                "$\\alpha \\leftarrow 0$ <br>\n",
                "$t\\leftarrow 1$ <br>\n",
                "$\\beta \\leftarrow +\\infty$ (`beta=np.inf`) <br>\n",
                "REPETIR<br>\n",
                "&nbsp;&nbsp;&nbsp;&nbsp; SI $f(\\bar{x}+td) > f(\\bar{x})+c_1t\\nabla f(\\bar{x})^Td$ :<br>\n",
                "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\beta\\leftarrow t$ <br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $t\\leftarrow \\frac{1}{2}(\\alpha+\\beta)$<br>\n",
                "&nbsp;&nbsp;&nbsp;&nbsp; De lo contrario, si $\\nabla f (\\bar{x} + td)^Td < c_2 \\nabla f(\\bar{x})^T d$:<br>\n",
                "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\alpha\\leftarrow t$ <br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $t\\leftarrow\\begin{cases} 2\\alpha \\quad \\text{si } \\beta=+\\infty \\\\ \\frac{1}{2}(\\alpha+\\beta) \\quad \\text{c.c.} \\end{cases} $ <br>\n",
                "&nbsp;&nbsp;&nbsp;&nbsp; Si no:<br>\n",
                "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; PARAR (`break`)<br>\n",
                "DEVOLVER $\\;t$\n",
                "\n",
                "\n",
                "Valores usuales para parámetros: $c_1= 0.5,\\; c_2=0.75$ <br>\n",
                "El comando `break` de Python permite salir de un ciclo `while` o `for`"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "81d1ebe9-0ac9-4526-a668-b173eef59c49",
                "id": "WMAp6ado3SuC",
                "pycharm": {}
            },
            "source": [
                "__Lema:__ sean $f:\\mathbb{R}^n\\rightarrow \\mathbb{R}$, $f\\in C^1$, $\\bar{x}\\in\\mathbb{R}^n$ y $d$ una dirección de descenso, entonces una de las siguientes dos situaciones pueden ocurrir con el método antes expuesto para las condiciones de Wolfe: <br>\n",
                "i) El procedimiento termina en una cantidad finita de pasos, devolviendo un valor $\\bar{t}$ que satisface las condiciones de Wolfe <br>\n",
                "ii) El procedimiento no termina: el parámetro $\\beta$ nunca toma un valor finito, $\\alpha$ se vuelve positivo en la primera iteración y se duplica con las iteraciones siguientes, y $f(x+td)\\rightarrow -\\infty$"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "cf50c96c-9c17-46aa-8f74-ba24a19e61ad",
                "id": "4oaN30ms3SuC",
                "pycharm": {}
            },
            "source": [
                "### Interpretación gráfica (2da. Condición de Wolfe)\n",
                "\n",
                "![](https://drive.google.com/uc?export=view&id=1Gqhma-ARLkuaYMWklImcQeB8etjfGlpb)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "83c2e69c-8a3c-48bf-b94a-cbb5bf976041",
                "id": "fEmLBFNZ3SuC",
                "pycharm": {}
            },
            "source": [
                "### Interpretación gráfica (Condiciones de Wolfe)\n",
                "\n",
                "![](https://drive.google.com/uc?export=view&id=1Zpz1yQTgyl3OGayHcvESBdoWKqlbmHdm)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "1ce1df16-2d1b-42e7-bb92-65966de2c894",
                "id": "N-R01gNVDa8I"
            },
            "source": [
                "### Ejercicio\n",
                "\n",
                "1. Implementar la funcion `wolfe` que encuentre un punto en la dirección $d$ que cumpla con las condiciones de Wolfe"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 50,
            "metadata": {
                "azdata_cell_guid": "a6bfb3cc-73e7-42f8-bc90-75e4b7cc5927",
                "id": "Ss5iMCIaDa8L",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "def wolfe(func, x, d, c1=0.5, c2=0.75):\n",
                "  \"\"\"\n",
                "  Aplica la búsqueda de Wolfe para hallar un valor de t que cumpla las condiciones de Wolfe para f(x+td)\n",
                "  func: funcion a optimizar (function)\n",
                "  x: punto a partir del cual buscar un minimizador (numpy.array)\n",
                "  d: direccion en la que se busca un minimizador (numpy.array)\n",
                "  c1: parámetro que regula la pendiente de la condición de Armijo (int o float)\n",
                "  c2: parámetro que regula la segunda condición de Wolfe (int o float)\n",
                "  \"\"\"\n",
                "  alfa = 0\n",
                "  t = 1\n",
                "  beta = np.inf\n",
                "\n",
                "  while 1 == 1:\n",
                "    if func(x + t*d) > func(x) + c1*t*(gradiente(func, x)@d):\n",
                "      beta = t\n",
                "      t = 0.5*(alfa + beta)\n",
                "    elif gradiente(func, x + t*d)@d < c2*(gradiente(func, x)@d):\n",
                "      alfa = t\n",
                "      t = 2*alfa if beta == np.inf else 0.5*(alfa + beta)\n",
                "    else:\n",
                "      break\n",
                "\n",
                "  return t"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "a41c596e-3e8e-4b2b-aacb-80634fdb691d",
                "id": "cFyPF9dbDa8N"
            },
            "source": [
                "2. Poner a prueba la función implementada en el ítem anterior para hallar un valor de $t$ que cumpla las condiciones de Wolfe, para $f(x)=x_1^2+x_2$ desde el punto $(1, 0)$ en la dirección $d=(-2,-1)$."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 51,
            "metadata": {
                "azdata_cell_guid": "3bad6ec3-6b64-428b-8c7d-30e6e620f7ac",
                "id": "4zHN6qMDDa8O",
                "language": "python"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "0.5\n"
                    ]
                }
            ],
            "source": [
                "result = wolfe(f, x, dire)\n",
                "\n",
                "print(result)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "2a36fd04-8112-4eae-aedd-543e237fc1ab",
                "id": "e_bi3Pxq3SuD",
                "pycharm": {}
            },
            "source": [
                "# Método del Gradiente - Funciones $C^1$"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "981baef8-65cb-44c7-ad7a-48616983ee1e",
                "id": "1vlrAl2h3SuD",
                "pycharm": {}
            },
            "source": [
                "__Algoritmo del Método del Gradiente__\n",
                "\n",
                "Dados: $f, x \\in \\mathbb{R}^n,\\; \\varepsilon>0,\\; k_{MAX}>0 $ <br>\n",
                "$k \\leftarrow 0$ <br>\n",
                "$d \\leftarrow -\\nabla f(x)$ <br>\n",
                "REPETIR mientras $\\lVert d\\rVert > \\varepsilon$ y $k<k_{MAX}$ : <br>\n",
                "&nbsp;&nbsp;&nbsp;&nbsp; Obtener $t>0$ tal que $f(x+td)<f(x)$ (con Armijo o Wolfe) <br>\n",
                "&nbsp;&nbsp;&nbsp;&nbsp; $x \\leftarrow x + td$ <br>\n",
                "&nbsp;&nbsp;&nbsp;&nbsp; $d \\leftarrow -\\nabla f(x)$ <br>\n",
                "&nbsp;&nbsp;&nbsp;&nbsp; $k \\leftarrow k+1$<br>\n",
                "DEVOLVER $x$\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "f074b98f-91a3-4cff-8f0a-bbe35b281451",
                "id": "9C0Ny49NBiWb"
            },
            "source": [
                "### Ejercicios"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "13ec7935-551a-4837-b3e7-dad281224f97",
                "id": "kGipJ93vCxTS"
            },
            "source": [
                "1. Implementar la funcion `metodo_gradiente` que aplique el método a una función a partir de un punto inicial dado. También debe tomar como argumento el método de búsqueda (Armijo o Wolfe)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "azdata_cell_guid": "6e16ad2a-d9f3-4ab0-a5d4-f1d6f41a02b2",
                "id": "8H2pryhbDkKm",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "def metodo_gradiente(func, x, eps=1e-5, k_max=1000, metodo=armijo):\n",
                "  \"\"\"\n",
                "  Aplica el método del gradiente.\n",
                "  func: funcion a optimizar (function)\n",
                "  x: punto inicial (numpy.array)\n",
                "  eps: valor de tolerancia para la norma del gradiente (float)\n",
                "  k_max: limite de iteraciones (int)\n",
                "  metodo: funcion a utilizar para la búsqueda en la diracción d (function)\n",
                "  \"\"\"\n",
                "  # COMPLETAR\n",
                "  return x"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "f4bc3dd7-e278-4977-ae35-0f01b1526a73",
                "id": "lJsURboUBnss"
            },
            "source": [
                "2. **Regresión lineal**. Dado un conjunto de datos $\\{(x_i,y_i)\\}_{i=1}^n$ queremos encontrar la ecuación de la recta $y=\\beta_1x + \\beta_0$ que mejor aproxime a los datos en sentido de cuadrados mínimos. Es decir, queremos hallar $(\\beta_0,\\beta_1)\\in\\mathbb{R}^2$ que minimice:\n",
                "$$LSE(\\beta_0,\\beta_1)=\\sum_{i=1}^n (y_i-(\\beta_1x_i+\\beta_0))^2$$\n",
                "Buscaremos la recta que mejor aproxima en sentido de cuadrados mínimos a datos sobre el valor de inmuebles en función de su superficie.\n",
                "\n",
                "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; a. Utilizar `lambda` y `sum` para definir la función a optimizar.\n",
                "\n",
                "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; b. Buscar la recta que mejor aproxima en sentido de cuadrados mínimos a los datos. Graficar el resultado para corroborarlo. Probar inicializando el método desde distintos puntos. Pueden usar `np.random.rand(2)` para generar vectores aleatorios en $[0,1]\\times[0,1]$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "azdata_cell_guid": "9f828da7-0f4d-46ff-9a2f-c9a3e39b866b",
                "id": "F_mhxKMnBhyE",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import seaborn.objects as so\n",
                "from sklearn.preprocessing import MinMaxScaler\n",
                "\n",
                "# A)\n",
                "\n",
                "# Importamos los datos desde una dirección web\n",
                "datos = pd.read_csv('https://raw.githubusercontent.com/fcen-amateur/ldd/refs/heads/main/Datos/inmuebles.csv', header=0)\n",
                "\n",
                "# Reescalamos los datos para favorecer la estabilidad numerica\n",
                "scaler = MinMaxScaler()\n",
                "datos_escalados = scaler.fit_transform(datos[['superficie', 'precio']])\n",
                "\n",
                "# Guardamos los valores de las variables en x e y\n",
                "datos_x = datos_escalados[:,0]\n",
                "datos_y = datos_escalados[:,1]\n",
                "\n",
                "# Definir la funcion a optimizar\n",
                "lse = lambda beta:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "azdata_cell_guid": "d3b20d75-5a2d-4cf7-83a2-65863300a73d",
                "id": "t2syBXUaIkMw",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "# B)\n",
                "\n",
                "x = # COMPLETAR\n",
                "beta = metodo_gradiente(lse, x)\n",
                "\n",
                "# Para graficar el beta obtenido:\n",
                "(so.Plot()\n",
                ".add(so.Dot(), x=datos_x, y=datos_y)\n",
                ".add(so.Line(color='red'), x=datos_x, y=beta[1]*datos_x+beta[0])\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "azdata_cell_guid": "df170b7b-83fc-49f3-92f4-e26f2466996a",
                "id": "sR0UKVmcH4HA",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "# Para graficar en los datos sin escalar\n",
                "\n",
                "# Deshacemos la escala\n",
                "datos_recta = scaler.inverse_transform(np.column_stack([datos_x, beta[1]*datos_x+beta[0]]))\n",
                "\n",
                "# Graficamos\n",
                "(so.Plot()\n",
                ".add(so.Dot(), x=datos['superficie'], y=datos['precio'])\n",
                ".add(so.Line(color='red'), x=datos_recta[:,0], y=datos_recta[:,1])\n",
                ")"
            ]
        }
    ],
    "metadata": {
        "celltoolbar": "Slideshow",
        "colab": {
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
